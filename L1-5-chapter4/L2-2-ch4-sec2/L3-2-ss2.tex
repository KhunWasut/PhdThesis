\begin{spacing}{2.0}
    \subsection{Inference of the Conditional Expectation}

    Due to the fact that $f$ satisfies the conditions of the Gaussian Process and elements of $\mathbf{f}$ from equation \ref{eq:gpr-arbitrary-function-vector}
    have a joint Gaussian distribution, if we apply the same function $f$ on the inputs of the test set $\mathcal{T}$, we would have a vector
    $\mathbf{f}^* = \left[f(\mathbf{x}_1^*)\,f(\mathbf{x}_2^*)\,\ldots\,f(\mathbf{x}_{N_{\mathcal{T}}}^*)\right]^{\top}$ whose elements also have a 
    joint Gaussian distribution. Therefore, \cite{B-GaussianProcessML-Rasmussen}

    \begin{equation}
        % LEFT VECTOR
        \left.\begin{bmatrix}
            \mathbf{f} \\ \mathbf{f}^*
        \end{bmatrix}\right|\mathbf{X},\mathbf{X}^* \sim \mathcal{N}\left(0,
            \begin{bmatrix}
                \mathbf{K}(\mathbf{X},\mathbf{X}) & \mathbf{K}(\mathbf{X},\mathbf{X}^*) \\
                \mathbf{K}(\mathbf{X}^*,\mathbf{X}) & \mathbf{K}(\mathbf{X}^*,\mathbf{X}^*)
            \end{bmatrix}
        \right)
        \label{eq:gpr-distribution-gaussian-process}
    \end{equation}

    Let $\epsilon = \left[\epsilon_1\,\epsilon_2\,\ldots\,\epsilon_N\right]^{\top}$, the vectorized form of equation \ref{eq:gpr-training-data-model} can
    be written as follow,

    \begin{equation}\begin{aligned}
        \mathbf{y} &= \mathbf{f} + \epsilon \\
        \mathbf{y}^* &= \mathbf{f}^* + \epsilon^*
    \end{aligned}\end{equation}

    Since we have defined that each element of $\mathbf{f}$ and $\mathbf{f}^*$ is a Gaussian random variable, and the elements of $\epsilon$ and $\epsilon^*$
    is also a Gaussian random variable as well, each element of $\mathbf{y}$ and $\mathbf{y}^*$ must also be a Gaussian random variable. Hence, similar to 
    equation \ref{eq:gpr-distribution-gaussian-process}, we could also express the joint Gaussian distribution between $\mathbf{y}$ and $\mathbf{y}^*$ as,

    \begin{equation}
        \left.\begin{bmatrix}
            \mathbf{y} \\ \mathbf{y}^*
        \end{bmatrix}\right|\mathbf{X},\mathbf{X}^* \sim \mathcal{N}\left(0,
            \begin{bmatrix}
                \mathbf{K}(\mathbf{X},\mathbf{X}) + \Sigma^2 I & \mathbf{K}(\mathbf{X},\mathbf{X}^*) \\
                \mathbf{K}(\mathbf{X},\mathbf{X}) & \mathbf{K}(\mathbf{X}^*,\mathbf{X}^*) + (\Sigma^*)^2 I
            \end{bmatrix}
        \right)
        \label{eq:gpr-distribution-gaussian-process-y}
    \end{equation}

    \noindent where $\Sigma^2 I$ and $(\Sigma^*)^2 I$ are the diagonal matrices of the individual variance of each point in the training data and the test data, 
    respectively. Since both $\mathbf{y}$ and $\mathbf{y}^*$ have joint Gaussian distribution, we could compute the conditional expectation of $\mathbf{y}^*$ in 
    the test set using the rules of the joint Gaussian distribution of two Gaussian random variables, \cite{W-Stanford-GPR} 

    \begin{equation}
        \mathbb{E}\left[\mathbf{y}^*|\mathbf{y},\mathbf{X},\mathbf{X}^*\right] = \mathbf{K}(\mathbf{X}^*,\mathbf{X})
            \left[\mathbf{K}(\mathbf{X},\mathbf{X}) + \Sigma^2 I\right]^{-1}\mathbf{y}
        \label{eq:gpr-expectation}
    \end{equation}
\end{spacing}
