#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
\paragraph_spacing double
\noindent
Multidimensional Free Energy Computation
\end_layout

\begin_layout Standard
\paragraph_spacing double
\noindent
NOTE: THIS IS ACTUALLY CHAPTER 4 OF THE THESIS!!
\end_layout

\begin_layout Section
\paragraph_spacing double
\noindent
Importance of Free Energy Computation
\end_layout

\begin_layout Standard
\paragraph_spacing double
\noindent
The free energy of a chemical system governs the macroscopic behavior along
 with its thermal fluctuation including both the energetic and entropic
 influences, and the free energy landscape may contain one or more local
 minima representing configurations at the metastable states, which are
 the locations where the system spends a significant amount of time.
 The change in the system from the reactant to the product states, therefore,
 is represented in the free energy landscape as a transition from one metastable
 state to another.
 During the course of the reaction, the system needs to pass through the
 free energy barrier, where the probability of barrier crossing with respect
 to a metastable state labeled as state 1 is defined as,
\end_layout

\begin_layout Standard
\paragraph_spacing double
\noindent
\begin_inset Formula 
\[
p(1\to*)\propto e^{-\beta\Delta A_{1\to*}}
\]

\end_inset


\end_layout

\begin_layout Standard
Equation XX can be further rearranged such that for any configuration 
\begin_inset Formula $\mathbf{x}\in\Omega$
\end_inset

, the relative free energy of the configuration 
\begin_inset Formula $\mathbf{x}$
\end_inset

is related to the equilibrium probability of finding the configuration 
\begin_inset Formula $\mathbf{x}$
\end_inset

in the phase space,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
A(\mathbf{x})=-\beta^{-1}\ln p(\mathbf{x})
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
Hypothetically, if a simulation is performed for an infinitely large amount
 of time, one could easily compute 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 for any configurations and then the configurational free energy landscape
 can be mapped.
 Nevertheless, as governed by equation XX, the higher the value 
\begin_inset Formula $\Delta A_{1\to*}$
\end_inset

 is, the longer timescale it would take to go from state 1 across the barrier.
 With limited computational resources, standard molecular dynamics (MD)
 simulation can only go up to the millisecond timescale, while 
\shape italic
ab initio
\shape default
 molecular dynamics (AIMD) fares far worse in timescale, only in the picosecond
 timescale due to a much higher cost for expensive first principles calculations.
 Hence, rare barrier crossing events typically are not well-sampled due
 to the issue of far less timescale affordable by current computational
 capabilities, and an attempt to compute the free energy landscape from
 any unbiased MD simulations would result in a noisy representation in the
 barrier region, hindering our insights on the dynamics of the transition
 state.
 Figure XX shows the one-dimensional free energy of 
\backslash
ce{NaCl + 495H2O} computed from the probability of finding the system at
 the cation - anion separation (
\begin_inset Formula $r_{+-}$
\end_inset

) ranging from 2.5 to 7.0 
\backslash
r{A}.
 It is clear that the noise around 
\begin_inset Formula $r_{+-}=3.0$
\end_inset

 
\backslash
r{A} is a result of inadequate sampling, because figure XX was computed
 from an unbiased simulation for only 240 ns, and according to our results
 in XX, there were only YY crossing events registered, which is not enough
 for us to deduce the dynamics of the rare events with confidence.
\end_layout

\begin_layout Standard
[PLACEHOLDER FOR FIGURE]
\end_layout

\begin_layout Standard
\paragraph_spacing double
Being able to compute relatively noise-free free energy landscapes, thus,
 opens up new insights into several relevant chemical processes, as well
 as their underlying mechanisms, as these free energy landscapes offer us
 insights into the relationship between all the metastable states and the
 dynamics around the transition state of amy chemical reactions.
 Nevertheless, as a chemical system may contain up to thousands of atoms,
 determining the free energy as a function of thousands of phase space variables
 is highly complex and expensive.
 However, relative free energies can be computed by constraining the absolute
 free energies into the collective variable space.
 Consequently, the relative free energy 
\begin_inset Formula $A(\xi_{1},\xi_{2},\ldots,\xi_{D})$
\end_inset

 can be written as a function of 
\begin_inset Formula $D$
\end_inset

 collective variables, where 
\begin_inset Formula $D$
\end_inset

 is the dimensionality of the problem which relates to the number of collective
 variables hypothesized to involve in the process where the reaction coordinate
 is a hypothetical linear combination of these 
\begin_inset Formula $D$
\end_inset

 variables that correlates with the slowest motion of the system across
 the free energy barrier.
 When written in terms of the collective variables, the free energy at 
\begin_inset Formula $\left\{ \xi_{1}^{*},\xi_{2}^{*},\ldots,\xi_{D}^{*}\right\} $
\end_inset

 is related to the natural logarithm marginal probability density,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
A\left(\xi_{1}^{*},\xi_{2}^{*},\ldots,\xi_{D}^{*}\right)=-\beta\ln\int\prod_{i=1}^{D}\delta\left(\xi_{i}(\mathbf{x})-\xi_{i}^{*}\right)e^{-\beta V(\mathbf{x})}d\mathbf{x}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathbf{x}$
\end_inset

is a configuration snapshot.
 Therefore, 
\begin_inset Formula $A\left(\xi_{1}^{*},\xi_{2}^{*},\ldots,\xi_{D}^{*}\right)$
\end_inset

 is computed by integrating over all the possible snapshots in the simulation.
 By taking the derivative of equation XX, the gradient of 
\begin_inset Formula $A\left(\xi_{1}^{*},\xi_{2}^{*},\ldots,\xi_{D}^{*}\right)$
\end_inset

 is found to be related to the statistical average of the gradient of the
 potential 
\begin_inset Formula $V(\mathbf{x})$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{\mathbf{x}}A=\left\langle \nabla_{\mathbf{x}}V\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
Equations XX and YY imply that there exists free energy estimator, that
 is, a monomer unit that can be used to deduce the bigger picture of the
 free energy landscape.
 For equation XX, the free energy estimator is a local probability density
 
\begin_inset Formula $p_{i}(\mathbf{x})$
\end_inset

, whereas for equation YY, the free energy estimator is a mean force 
\begin_inset Formula $f_{i}=-\left\langle \nabla_{\mathbf{x}}V(\mathbf{x}_{i})\right\rangle $
\end_inset

.
 In order to get the free energy estimators for each part of the phase space,
 extensive sampling of that part is required to ensure statistical viability
 of the free energy estimators.
 Hence, the earliest approaches to compute free energy estimators came from
 constraining the simulations into several windows, where the local probability
 density or the mean force for each window can be computed using techniques
 such as Umbrella Sampling (US) for biased local probability densities,
 Thermodynamic Integration (TI) for the mean forces to be integrated into
 the free energy either through fixing the atoms involving in the reaction
 coordinates using SHAKE algorithm, or using a stiff harmonic restraint
 to limit sampling around the center of the windowwindow, which is better
 known as Umbrella Integration (UI) technique.
 Another approach to sample the CV space is through the holonomic constraints,
 which is used to confine the system to the collective variable hypersurface
 such as Blue Moon Sampling.
 However, the major disadvantage of the aforementioned techniques is the
 computational cost.
 To minimize possible statistical uncertainties of the estimators, a long
 simulation in each window is necessary for adequate sampling of any part
 of the collective variable space we desire to explore, including the rare
 event regions.
 Therefore, these kinds of simulations are costly due to the need to minimize
 sampling errors.
 There are two main kinds of error associating with the process.
 First of which is the statistical error due to the number of samples, and
 second of which is the error due to step size (i.e.
 the width of the window).
 In order to minimize these errors, one needs to reduce the width of window
 to reduce the error from the large step sizes, as well as performing very
 long simulation in each of the window to reduce the error from inaqequate
 sampling.
 While this is doable in one dimension, the cost to obtain samples in a
 
\begin_inset Formula $D$
\end_inset

 dimensional problem usually scales as 
\begin_inset Formula $\mathcal{O}(N_{window}^{D})$
\end_inset

, where 
\begin_inset Formula $N_{window}$
\end_inset

 is the number of windows usually required to obtain an acceptable result
 in one dimension.
 Moreover, as the dimensionality of the problem increases, the sampling
 area becomes larger, which necessitates even longer simulation per sampling
 area, further increasing total computational cost.
 Therefore, with limited computational resources, windowed simulations for
 multidimensional problems are inherently expensive and takes very long
 time even for classical MD simulations.
\end_layout

\begin_layout Standard
\paragraph_spacing double
In the previous decade, numbers of methods were introduced to selectively
 apply biases along the collective variable spaces in order to force the
 exploration away from free energy minima.
 Adaptive Biasing Force (ABF) makes use of sampling local free energy gradients
 and use it as the biasing force that pushes the dynamics away from the
 minima.
 Temperature-Accelerated Molecular Dynamics (TAMD) manipulates the dynamics
 in the collective variable space to make it faster than the actual coordinate
 space, and let the faster dynamics of the collective variable space pull
 the actual dynamics away from the minima, and Metadynamics (MTD) periodically
 adds repulsive bias potential along the visited regions in the collective
 variable space, thereby enhancing the rare event sampling frequency.
 These three methods can also be used to directly approximate the free energy
 in the collective variable space at very long time limit.
 While using these methods to compute the free energy sounds attractive,
 the main drawback is that the free energy can only be recovered at very
 long time limit.
 [MTD DRAWBACKS HERE] Also, methods like MTD or TAMD requires parameters
 adjustment, where a bad set of parameters may never give a good answer
 to the problems.
 However, these methods show potential uses as tools to quickly explore
 the CV space, as shown in the work by Maragliano and Vanden-Eijnden, as
 well as Cuendet and Tuckerman.
\end_layout

\begin_layout Standard
\paragraph_spacing double
In order to efficiently compute free energy landscapes, the key challenges
 of the methods mentioned in the above paragraphs, such as the need to compute
 local probability densities or the mean forces in sampling cases or the
 need to let the dynamics asymtotically converge to the free energy at very
 long time presented by the adaptive methods, need to be addressed.
 Recently, machine learning has become a buzzword in a scientific and engineerin
g community due to active fundings by large enterprises with huge computational
 resources driven by the need to predict underlying patterns in large amount
 of available data.
 In chemistry, it has been used in various applications; for example, approximat
ing ab initio energies, deriving newer force field models, or structural
 characterization of biomolecules or advanced materials.
 In free energy computation, Gaussian Process Regression (GPR) and Artificial
 Neural Networks (ANN) have successfully been applied for various polypeptide
 computational models with up to 8 dimensions using dihedral angles as the
 collective variables.
 However, GPR has been around for a significantly longer time, and it has
 recently been used in a one-dimensional free energy computation from an
 expensive AIMD simulation as the first example beyond polypeptide computational
 models.
 Nevertheless, its recent application in free energy computations implies
 that there is no established simulation protocols to be based on, especially
 for multidimensional problems in chemically reactive systems.
\end_layout

\begin_layout Standard
\paragraph_spacing double
GPR addresses the key challenges mentioned earlier by entirely eliminating
 the need to obtain statistical averages for free energy estimators by assuming
 statistical noises associated with each estimator is part of the procedure,
 and the conditional expectation of the free energy based on the available
 information of the estimators and the explored CV can be computed from
 the instantaneous force free energy estimators without the need to perform
 simulations for very long time, provided that the CV space of interest
 is adequately explored.
 Recent work by Mones et al.
 shows that the fastest exploration of the CV space can be achieved by using
 well-tempered metadynamics (WT-MTD) simulation with relatively long Gaussian
 deposition rate to ensure the quasi-equilibrium condition and a high bias
 factor to quickly encourage the system to quickly overcome the free energy
 barriers, which has an effect of giving a noisy reconstruction of the free
 energy landscape.
 As a result, one can construct multidimensional free energy landscapes
 with much less efforts due to the elimination of the need to sample 
\begin_inset Formula $N_{window}^{D}$
\end_inset

 areas in the CV space and the entire simulation was transformed into a
 single biased simulation that seeks to explore the CV space as quickly
 as possible, while a reconstruction of multidimensional free energy surface
 was fitted according to the maximum likelihood of the non-averaged instantaneou
s forces in the CV space as free energy estimators to obtain the best fit
 to the simulation data.
 In the next section, the theoretical aspect of GPR will be discussed in
 detail, as well as our proposed protocol for effective free energy reconstructi
on with GPR while ensuring a good agreement with traditional free energy
 computation methods while keeping the computational cost minimal.
 With our protocol, there are huge future implications for any kinds of
 computationally expensive problems.
\end_layout

\begin_layout Section
Gaussian Process Regression (GPR)
\end_layout

\begin_layout Subsection
Training Data
\end_layout

\begin_layout Standard
\paragraph_spacing double
Gaussian process regression (GPR) is a class of machine learning algorithms,
 where one aims to make a prediction of the relationship between one set
 of input values to another set of output values based on the provided 
\shape italic
training data
\shape default
 
\begin_inset Formula $\mathcal{D}$
\end_inset

, which is a set of data obtained from observations or experimental evidence.
 A set of the training data 
\begin_inset Formula $\mathcal{D}$
\end_inset

 with 
\begin_inset Formula $N$
\end_inset

 training examples is written as 
\begin_inset Formula $\mathcal{D}=\left\{ (x_{i},y_{i})\right\} _{i=1}^{N}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is a 
\begin_inset Formula $1\times D$
\end_inset

 row vector called a feature vector with 
\begin_inset Formula $D$
\end_inset

 features.
 The main objective of any machine learning algorithm is to recover the
 underlying relationship 
\begin_inset Formula $f$
\end_inset

 that maps 
\begin_inset Formula $\mathbf{X}=\left[\mathbf{x}_{1}\,\mathbf{x}_{2}\,\ldots\,\mathbf{x}_{N}\right]^{T}$
\end_inset

 to 
\begin_inset Formula $\mathbf{y}=\left[y_{1}\,y_{2}\,\ldots\,y_{N}\right]^{T}$
\end_inset

.
 The predicted form of 
\begin_inset Formula $f$
\end_inset

 would then be used to make a prediction in a test set with 
\begin_inset Formula $N_{T}$
\end_inset

 entries 
\begin_inset Formula $\mathcal{T}=\left\{ (\mathbf{x}_{i}^{*},y_{i}^{*})\right\} _{i=1}^{N_{T}}$
\end_inset

 with an assertion that for any 
\begin_inset Formula $\mathbf{x}_{i}^{*},y_{i}^{*}\in\mathcal{T}$
\end_inset

, each value of 
\begin_inset Formula $y_{i}^{*}$
\end_inset

 is a conditional mapping of 
\begin_inset Formula $\mathbf{x}_{i}^{*}$
\end_inset

 by 
\begin_inset Formula $f:\mathbb{R}^{D}\to\mathbb{R}$
\end_inset

 such that 
\begin_inset Formula $\left.y_{i}^{*}=f(\mathbf{x}_{i}^{*})\right|\mathcal{D}$
\end_inset

.
 GPR itself is a subset of a class of machine learning algorithms called
 Bayesian Concept Learning, where the main concept is to compute a conditional
 expectation of 
\begin_inset Formula $f$
\end_inset

 given a training data 
\begin_inset Formula $\mathcal{D}$
\end_inset

 based on Bayesian inference.
 In GPR, the 
\begin_inset Formula $i$
\end_inset

-th output value of the training data 
\begin_inset Formula $y_{i}$
\end_inset

 is assumed to approximately relate to the input features 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 through a mapping 
\begin_inset Formula $f:\mathbb{R}^{D}\to\mathbb{R}$
\end_inset

.
 Each of the value 
\begin_inset Formula $y_{i}$
\end_inset

, however, is not the exact mapping of corresponding 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

, and each 
\begin_inset Formula $y_{i}$
\end_inset

 has a mapping error 
\begin_inset Formula $\epsilon_{i}$
\end_inset

.
 Therefore, we could express the relationship between 
\begin_inset Formula $y_{i}$
\end_inset

 and 
\begin_inset Formula $f(\mathbf{x}_{i})$
\end_inset

 as follow,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{i}=f(\mathbf{x}_{i})+\epsilon_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
The deviation 
\begin_inset Formula $\epsilon_{i}$
\end_inset

 can be viewed as inherent statistical errors of the experiment, and in
 GPR, it is assumed to have a normal distribution 
\begin_inset Formula $\epsilon_{i}\sim\mathcal{N}(0,\sigma_{\epsilon_{i}}^{2})$
\end_inset

.
 To determine the underlying 
\begin_inset Formula $f$
\end_inset

 that maps 
\begin_inset Formula $\mathbf{X}$
\end_inset

 to 
\begin_inset Formula $\mathbf{y}$
\end_inset

, let us define a vector 
\begin_inset Formula $\hat{f}$
\end_inset

where each element represents the evaluation of some arbitrary function
 
\begin_inset Formula $f$
\end_inset

 that we hope to represent the actual relationship we are looking for each
 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{f}=\left[f(\mathbf{x}_{1})\,f(\mathbf{x}_{2})\,\ldots\,f(\mathbf{x}_{N})\right]^{\top}
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
If the elements of 
\begin_inset Formula $\mathbf{f}$
\end_inset

 in equation XX have a joint Gaussian distribution, then the function 
\begin_inset Formula $f$
\end_inset

 that gives rise to the aforementioned property is said to be a Gaussian
 Process 
\begin_inset Formula $f\sim\mathcal{GP}\left(m(\cdot),k(\cdot,\cdot)\right)$
\end_inset

 with mean 
\begin_inset Formula $m$
\end_inset

 and covariance 
\begin_inset Formula $k$
\end_inset

.
 For the purpose of approximating the conditional expectation of 
\begin_inset Formula $f$
\end_inset

, the mean of this Gaussian Process is set to zero.
 The covariance of the Gaussian Process can be approximated by a kernel
 where the key property of the covariance represents the correlation between
 two points in the feature space 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{j}$
\end_inset

.
 If the two points are identical, then the correlation should be at its
 maximum.
 If the two points are very far apart, then the correlation is expected
 to decay towards zero, or no correlations at all.
 To satisfy this property, the most common choice of the covariance approximatio
n kernel is the squared exponential kernel 
\begin_inset Formula $\mathbf{K}_{SE}$
\end_inset

, which is a 
\begin_inset Formula $ND\times ND$
\end_inset

 matrix whose elements are defined as follow,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k_{ij}=\mathbf{K}(\mathbf{x}_{i},\mathbf{x}_{j})=\chi^{2}\exp\left(-\frac{1}{2}\sum_{a=1}^{D}\frac{\left(\mathbf{x}_{i}^{(a)}-\mathbf{x}_{j}^{(a)}\right)^{2}}{\left(\lambda^{(a)}\right)^{2}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing double
where 
\begin_inset Formula $\chi$
\end_inset

 represents the overall deviation of the function value in the region of
 interest, and 
\begin_inset Formula $\lambda^{(a)}$
\end_inset

 is called the 
\backslash
textsl{length scale} in the 
\begin_inset Formula $a$
\end_inset

-th dimension.
 For a training data with 
\begin_inset Formula $D$
\end_inset

 dimensions, there are 
\begin_inset Formula $D$
\end_inset

 values of the length scale, each of which controls how abrupt the correlation
 between the two points in the feature space should be in a specific dimension.
 Thus, if a dimension has a small value of 
\begin_inset Formula $\lambda^{(a)}$
\end_inset

, it means that the value of the function should vary more rapidly in that
 dimension, and if a dimension has a large value of 
\begin_inset Formula $\lambda^{(a)}$
\end_inset

, it means that the value of the function would vary slowly in that dimension.
 Equations XX and YY are, therefore, key ingredients to the GPR modeling
 of the test set, and the success of such modeling would require a good
 set of 
\begin_inset Formula $2D+1$
\end_inset

 parameters containing 
\begin_inset Formula $D$
\end_inset

 values of 
\begin_inset Formula $\sigma_{\epsilon_{i}}^{2}$
\end_inset

, 
\begin_inset Formula $D$
\end_inset

 values of 
\begin_inset Formula $\lambda^{(a)}$
\end_inset

, and one value for 
\begin_inset Formula $\chi$
\end_inset

.
 These parameters are called hyperparameters in machine learning literatures,
 and an optimum set of hyperparameters should result in a maximum likelihood
 
\begin_inset Formula $p(\mathcal{D}|\mathbf{y}^{*},\mathbf{X}^{*})$
\end_inset

 that represents the best fit to the training data.
 However, Stecher et al.
 suggested that hyperparameters optimization is a computationally expensive
 task, but one could make a good choice of the hyperparameters from the
 a priori knowledge of our training data.
 Moreover, in the application for free energy surface reconstruction, the
 range of good hyperparameters can vary at a large range while does not
 result in a significantly different reconstructed free energy landscape.
\end_layout

\begin_layout Standard
Due to the fact that 
\begin_inset Formula $f$
\end_inset

 satisfies the conditions of the Gaussian Process and elements of 
\begin_inset Formula $\mathbf{f}$
\end_inset

 from equation XX have a joint Gaussian distribution, if we apply the same
 function f on the inputs of the test set, we would have a vector 
\begin_inset Formula $\mathbf{f}^{*}=\left[f(\mathbf{x}_{1}^{*})\,f(\mathbf{x}_{2}^{*})\,\ldots\,f(\mathbf{x}_{N}^{*})\right]^{\top}$
\end_inset

 whose elements also have a joint Gaussian distribution.
 Therefore,
\end_layout

\begin_layout Standard
[THE MATRIX IS ALREADY IN ACTUAL THESIS]
\end_layout

\begin_layout Standard
Now define 
\begin_inset Formula $\epsilon=\left[\epsilon_{1}\,\epsilon_{2}\,\ldots\,\epsilon_{N}\right]^{\top}$
\end_inset

, the vector form of equation XX for the training set 
\begin_inset Formula $\mathcal{D}$
\end_inset

 and for the test set 
\begin_inset Formula $\mathcal{T}$
\end_inset

 can be written as,
\end_layout

\begin_layout Standard
[THE EQUATION IS ALREADY IN ACTUAL THESIS]
\end_layout

\begin_layout Standard
Since we have defined that each element of 
\begin_inset Formula $\mathbf{f}$
\end_inset

 and 
\begin_inset Formula $\mathbf{f}^{*}$
\end_inset

 is a Gaussian random variable, and the elements of 
\begin_inset Formula $\epsilon$
\end_inset

 and 
\begin_inset Formula $\epsilon^{*}$
\end_inset

 are also Gaussian random variable as well, each element of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}^{*}$
\end_inset

 must also be Gaussian random variable.
 Hence, similar to equation XX, we could also express the joint Gaussian
 distribution between 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}^{*}$
\end_inset

 as,
\end_layout

\begin_layout Standard
[THE EQUATION IS ALREADY IN ACTUAL THESIS]
\end_layout

\begin_layout Standard

\backslash
noindent where 
\begin_inset Formula $\Sigma^{2}I$
\end_inset

 and 
\begin_inset Formula $(\Sigma^{*})^{2}I$
\end_inset

 are the diagonal matrices of the individual variance of each point in the
 training data and the test data, respectively.
 Since both 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}^{*}$
\end_inset

 have joint Gaussian distribution, we could compute the conditional expectation
 of 
\begin_inset Formula $\mathbf{y}^{*}$
\end_inset

 in the test set using the rules of the joint Gaussian distribution of two
 Gaussian random variables 
\end_layout

\begin_layout Standard
[THE EQUATION IS ALREADY IN ACTUAL THESIS]
\end_layout

\begin_layout Standard
Besides being able to predict the expectation of the functions in the test
 set from the training data from the function itself, GPR is also able to
 predict the expectation of the functions in the test set from the training
 data of its partial derivatives with respect to each of the individual
 features while also including the inherent statistical errors in all dimensions.
 As the derivative of Gaussian Process is still a Gaussian Process, we could
 write a vector 
\begin_inset Formula $\mathbf{f}'=\left[f'(\mathbf{x}_{1})\,f'(\mathbf{x}_{2})\,\ldots\,f'(\mathbf{x}_{N})\right]^{\top}$
\end_inset

 such that each element of 
\begin_inset Formula $\mathbf{f}'$
\end_inset

 is also a Gaussian random variable.
 Thus, it is possible to substitute 
\begin_inset Formula $\mathbf{f}$
\end_inset

 in equation XX with 
\begin_inset Formula $\mathbf{f}'$
\end_inset

,
\end_layout

\begin_layout Subsection
Inference of the Conditional Expectation
\end_layout

\begin_layout Standard
blah blah blah
\end_layout

\begin_layout Section
Using GPR to Compute Free Energy Landscapes
\end_layout

\begin_layout Subsection
Fast Exploration of the Collective Variable Space
\end_layout

\begin_layout Standard
blah blah blah
\end_layout

\begin_layout Subsection
GPR Reconstruction of Free Energy Landscapes from Noisy Training Data
\end_layout

\begin_layout Standard
blah blah blah
\end_layout

\begin_layout Subsection
Error Analysis of GPR-constructed Free Energy Landscapes
\end_layout

\begin_layout Standard
blah blah blah
\end_layout

\end_body
\end_document
